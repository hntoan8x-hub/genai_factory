# domain_models/genai_assistant/services/assistant_trainer.py (C·∫¨P NH·∫¨T HO√ÄN TO√ÄN)

import logging
from typing import Any, Dict, List
import asyncio
from shared_libs.utils.exceptions import GenAIFactoryError
from shared_libs.orchestrator.evaluation_orchestrator import EvaluationOrchestrator 

# üö® THAY TH·∫æ: Import tr·ª±c ti·∫øp Base Interfaces t·ª´ Shared MLOps Core
from shared_libs.mlops.base.base_tracker import BaseTracker 
from shared_libs.mlops.base.base_registry import BaseRegistry 
# X√≥a import: from domain_models.genai_assistant.logging.mlflow_adapter import MLflowAdapter 

# Gi·∫£ ƒë·ªãnh c√°c schemas kh√°c v·∫´n ƒë∆∞·ª£c gi·ªØ nguy√™n
from domain_models.genai_assistant.schemas.eval_schema import EvaluationResult, SafetyEvaluation 
from domain_models.genai_assistant.schemas.config_schemas import LLMConfigSchema 

logger = logging.getLogger(__name__)

class AssistantTrainer:
    """
    Service responsible for model fine-tuning, evaluation, and metric logging.
    Now directly uses MLOps Base Interfaces (Tracker, Registry) for full Abstraction.
    """

    def __init__(self, 
                 config: Dict[str, Any], 
                 # üö® C·∫¨P NH·∫¨T: Nh·∫≠n BaseTracker v√† BaseRegistry qua DI
                 tracker: BaseTracker, 
                 registry: BaseRegistry, 
                 eval_orchestrator: EvaluationOrchestrator):
        """
        Initializes the trainer with all necessary MLOps components injected.
        """
        self.config = config
        # THAY TH·∫æ: L∆∞u tr·ªØ BaseTracker v√† BaseRegistry
        self.tracker = tracker
        self.registry = registry
        self.eval_orchestrator = eval_orchestrator
        
        # L·∫•y ng∆∞·ª°ng ch·∫•t l∆∞·ª£ng t·ª´ config (CRITICAL HARDENING PARAMETER)
        self.quality_threshold = config.get("quality_threshold", 0.75)
        self.safety_threshold = config.get("safety_threshold", 0.95)
        self.hallucination_max = config.get("hallucination_max_rate", 0.10)


    def _validate_metrics_against_gate(self, metrics: List[Dict[str, Any]]) -> bool:
        """
        Runs the Model Quality Gate against the collected metrics.
        (Logic n√†y gi·ªØ nguy√™n)
        """
        metrics_map = {m['metric_name']: m for m in metrics}
        
        # Ki·ªÉm tra An to√†n (Safety)
        safety_metric = metrics_map.get("toxicity_score")
        if safety_metric and safety_metric['score'] < self.safety_threshold:
            logger.critical(f"FAIL: Safety Score ({safety_metric['score']:.4f}) is below threshold ({self.safety_threshold}).")
            return False

        # Ki·ªÉm tra Ch·∫•t l∆∞·ª£ng (Coherence/BLEU - gi·∫£ ƒë·ªãnh Coherence l√† metric ch√≠nh)
        quality_metric = metrics_map.get("CoherenceScore")
        if quality_metric and quality_metric['score'] < self.quality_threshold:
            logger.critical(f"FAIL: Quality Score ({quality_metric['score']:.4f}) is below threshold ({self.quality_threshold}).")
            return False
            
        # Ki·ªÉm tra Hallucination (Gi·∫£ ƒë·ªãnh c√≥ Evaluator tr·∫£ v·ªÅ metric n√†y)
        hall_metric = metrics_map.get("hallucination_rate")
        if hall_metric and hall_metric['score'] > self.hallucination_max:
             logger.critical(f"FAIL: Hallucination Rate ({hall_metric['score']:.4f}) exceeds max rate ({self.hallucination_max}).")
             return False

        return True


    async def run_training_job(self, dataset_path: str, model_name: str, fine_tuning_params: Dict[str, Any], git_sha: str) -> str:
        """
        Runs the E2E Fine-Tuning and Evaluation cycle, logging and managing model versioning.
        
        Args:
            git_sha (str): SHA commit c·ªßa Git (d√πng cho Traceability).
            
        Returns: Path to the validated model artifact.
        """
        run_name = f"finetune-{model_name}-{dataset_path.split('/')[-1]}-{asyncio.current_task().get_name()}"
        logger.info(f"Starting traceable training job: {run_name}.")
        
        output_model_path = ""
        model_version = None
        
        try:
            # 1. MLOps Tracking Start (S·ª≠ d·ª•ng BaseTracker)
            # Context Manager ƒë·∫£m b·∫£o run k·∫øt th√∫c v√† ƒë√°nh d·∫•u tr·∫°ng th√°i ƒë√∫ng
            with self.tracker.start_run(run_name=run_name) as run:
                
                # Log parameters
                self.tracker.log_param("model_name", model_name)
                self.tracker.log_param("dataset_path", dataset_path)
                self.tracker.log_param("git_commit_sha", git_sha)
                self.tracker.log_metrics({"initial_lr": fine_tuning_params.get("learning_rate", 1e-5)})
                
                # --- 2. Model Fine-Tuning (Placeholder) ---
                logger.info("Finetuning process started...")
                # self._call_external_training_service(...) 
                # Gi·∫£ ƒë·ªãnh sau hu·∫•n luy·ªán c√≥ ƒë∆∞·ª£c m√¥ h√¨nh v√† log n√≥
                mock_model = {"model_data": "some_weights"} 
                output_model_path = f"model_artifact" # ƒê∆∞·ªùng d·∫´n artifact trong MLflow
                
                # Log Model Artifact (S·ª≠ d·ª•ng BaseTracker)
                self.tracker.log_model(model=mock_model, artifact_path=output_model_path)
                
                # --- 3. Evaluation (CRITICAL QUALITY CHECK) ---
                logger.info("Starting post-training evaluation...")
                test_data = [{"input": "q1", "ref": "a1"}] 
                raw_metrics = await self.eval_orchestrator.async_evaluate_batch(output_model_path, test_data)
                
                # 4. Log Metrics
                for metric in raw_metrics:
                    self.tracker.log_metrics({metric['metric_name']: metric['score']})
                    
                # 5. Deployment Decision Logic (HARDENING: Model Guard)
                if not self._validate_metrics_against_gate(raw_metrics):
                     # N·∫øu Quality Gate th·∫•t b·∫°i, n√©m l·ªói 
                     raise GenAIFactoryError("Model failed the mandatory quality and safety gates.")
                
                # 6. Model Registration & Promotion (S·ª≠ d·ª•ng BaseRegistry)
                model_uri = f"runs:/{run.info.run_id}/{output_model_path}"
                
                model_version = self.registry.register_model(
                    model_name=model_name, 
                    run_id=run.info.run_id, 
                    artifact_path=output_model_path,
                    description=f"Finetune run from Git SHA: {git_sha}"
                )
                
                # Tag Version v·ªõi th√¥ng tin truy v·∫øt (Traceability Hardening)
                self.registry.tag_model_version(
                    model_name=model_name, 
                    version=model_version.version, 
                    tags={"git_sha": git_sha, "passed_quality_gate": "true"}
                )
                
                # Chuy·ªÉn sang Staging (B·∫±ng retry logic t·ª´ BaseRegistry/MLflowRegistry)
                self.registry.transition_model_stage(
                    model_name=model_name, 
                    version=model_version.version, 
                    new_stage="Staging"
                )
                
                logger.info(f"Model {model_name} version {model_version.version} PASSED and moved to 'Staging'.")
                return model_uri

        except GenAIFactoryError as e:
            # L·ªói nghi·ªáp v·ª• (Quality Gate th·∫•t b·∫°i ho·∫∑c l·ªói kh·ªüi t·∫°o)
            logger.critical(f"MLOps Job failure: {e}")
            # Context manager c·ªßa BaseTracker s·∫Ω t·ª± ƒë·ªông ƒë√°nh d·∫•u RUN l√† FAILED
            raise 
            
        except Exception as e:
            # L·ªói k·ªπ thu·∫≠t kh√¥ng l∆∞·ªùng tr∆∞·ªõc ƒë∆∞·ª£c
            logger.critical(f"FATAL Training Job failure: Unhandled error: {e.__class__.__name__}", exc_info=True)
            raise GenAIFactoryError(f"Trainer failed during MLOps cycle: {e}") from e